#!/usr/bin/env python3
"""Remote CLI client for PDFreducer NAS service."""

import argparse
import json
import shutil
import sys
import time
import urllib.request
import urllib.error
from pathlib import Path
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email.mime.text import MIMEText
import uuid


DEFAULT_SERVER = "192.168.178.58:5052"


def create_multipart_form(pdf_path: Path, mode: str = "extract") -> tuple[bytes, str]:
    """Create multipart form data for file upload."""
    boundary = f"----WebKitFormBoundary{uuid.uuid4().hex[:16]}"

    body = []

    # Add mode field
    body.append(f"--{boundary}".encode())
    body.append(b'Content-Disposition: form-data; name="mode"')
    body.append(b"")
    body.append(mode.encode())

    # Add file field
    body.append(f"--{boundary}".encode())
    body.append(f'Content-Disposition: form-data; name="file"; filename="{pdf_path.name}"'.encode())
    body.append(b"Content-Type: application/pdf")
    body.append(b"")
    body.append(pdf_path.read_bytes())

    body.append(f"--{boundary}--".encode())
    body.append(b"")

    return b"\r\n".join(body), boundary


def upload_file(server: str, pdf_path: Path, mode: str = "extract") -> dict:
    """Upload a PDF file to the server."""
    url = f"http://{server}/api/upload"

    body, boundary = create_multipart_form(pdf_path, mode)

    req = urllib.request.Request(url, data=body)
    req.add_header("Content-Type", f"multipart/form-data; boundary={boundary}")

    try:
        with urllib.request.urlopen(req, timeout=300) as response:
            return json.loads(response.read().decode())
    except urllib.error.HTTPError as e:
        return {"error": f"HTTP {e.code}: {e.reason}"}
    except urllib.error.URLError as e:
        return {"error": str(e.reason)}
    except TimeoutError:
        return {"error": "upload timed out (file too large?)"}


def get_job_status(server: str, job_id: str) -> dict:
    """Get status of a specific job."""
    url = f"http://{server}/api/jobs/{job_id}"

    try:
        with urllib.request.urlopen(url, timeout=30) as response:
            return json.loads(response.read().decode())
    except Exception:
        return {"job": {"status": "pending"}}


def download_file(server: str, job_id: str, output_path: Path) -> bool:
    """Download the processed file."""
    url = f"http://{server}/api/download/{job_id}"

    try:
        with urllib.request.urlopen(url, timeout=120) as response:
            output_path.write_bytes(response.read())
            return True
    except Exception:
        return False


def start_processing(server: str) -> dict:
    """Trigger processing of all pending jobs."""
    url = f"http://{server}/api/process"

    try:
        req = urllib.request.Request(url, method="POST")
        with urllib.request.urlopen(req, timeout=30) as response:
            return json.loads(response.read().decode())
    except Exception:
        return {"error": "failed to start processing"}


def wait_for_completion(server: str, job_ids: list[str], timeout: int = 300) -> dict[str, dict]:
    """Wait for all jobs to complete."""
    results = {}
    start_time = time.time()
    pending = set(job_ids)

    while pending and (time.time() - start_time) < timeout:
        for job_id in list(pending):
            status = get_job_status(server, job_id)
            job = status.get("job", {})

            if job.get("status") == "completed":
                results[job_id] = job
                pending.remove(job_id)
                print(f"  ✓ {job.get('filename', job_id)}")
            elif job.get("status") == "failed":
                results[job_id] = job
                pending.remove(job_id)
                print(f"  ✗ {job.get('filename', job_id)}: {job.get('error', 'Failed')}")

        if pending:
            time.sleep(1)

    return results


def main():
    parser = argparse.ArgumentParser(
        description="Extract text from PDFs using remote PDFreducer server",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  pdfreducer-remote *.pdf
  pdfreducer-remote report.pdf --move-originals
  pdfreducer-remote *.pdf --server 192.168.1.100:5052
        """,
    )

    parser.add_argument(
        "files",
        nargs="+",
        type=Path,
        help="PDF files to process",
    )
    parser.add_argument(
        "--server",
        default=DEFAULT_SERVER,
        help=f"Server address (default: {DEFAULT_SERVER})",
    )
    parser.add_argument(
        "--move-originals",
        action="store_true",
        help="Move original PDFs to _originals/ subdirectory after extraction",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=300,
        help="Timeout in seconds (default: 300)",
    )

    args = parser.parse_args()

    # Filter to only existing PDF files
    pdf_files = [f for f in args.files if f.exists() and f.suffix.lower() == ".pdf"]

    if not pdf_files:
        print("No PDF files found")
        return 1

    print(f"Uploading {len(pdf_files)} file(s) to {args.server}...")

    # Upload all files
    job_ids = []
    file_map = {}  # job_id -> original path

    for pdf_path in pdf_files:
        try:
            result = upload_file(args.server, pdf_path)
            if "error" in result:
                print(f"  ✗ {pdf_path.name}: {result['error']}")
            else:
                job_id = result["job_id"]
                job_ids.append(job_id)
                file_map[job_id] = pdf_path
                print(f"  ↑ {pdf_path.name}")
        except Exception as e:
            print(f"  ✗ {pdf_path.name}: {e}")

    if not job_ids:
        print("No files uploaded successfully")
        return 1

    # Start processing
    print("\nProcessing...")
    start_processing(args.server)

    # Wait for completion
    results = wait_for_completion(args.server, job_ids, args.timeout)

    # Download completed files
    print("\nDownloading...")
    downloaded = []

    for job_id, job in results.items():
        if job.get("status") == "completed":
            original_path = file_map[job_id]
            output_path = original_path.parent / f"{original_path.stem}.txt"

            if download_file(args.server, job_id, output_path):
                print(f"  ↓ {output_path.name}")
                downloaded.append((job_id, original_path, output_path))
            else:
                print(f"  ✗ Failed to download {original_path.stem}.txt")

    # Move originals if requested
    if args.move_originals and downloaded:
        originals_dir = pdf_files[0].parent / "_originals"
        originals_dir.mkdir(exist_ok=True)

        print(f"\nMoving originals to {originals_dir}/")
        for job_id, original_path, output_path in downloaded:
            dest = originals_dir / original_path.name
            shutil.move(str(original_path), str(dest))
            print(f"  → {original_path.name}")

    # Summary
    print(f"\nDone: {len(downloaded)}/{len(pdf_files)} files extracted")

    return 0 if downloaded else 1


if __name__ == "__main__":
    sys.exit(main())
